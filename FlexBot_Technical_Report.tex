\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

% Colors
\definecolor{codeblue}{RGB}{0,102,204}
\definecolor{codegray}{RGB}{128,128,128}

% Listings style
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{codeblue},
    commentstyle=\color{codegray},
    breaklines=true,
    frame=single,
    captionpos=b
}

% Title
\title{
    \textbf{FlexBot: Sim2Real Robot Control with Domain Randomization} \\
    \large A Technical Report on Synthetic Data Generation for Robot Learning
}
\author{
    Seymur Hasanov
}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
This report presents an exploration of \textbf{Sim2Real transfer} techniques for robot manipulation using domain randomization and reinforcement learning. We implement a simulated robot arm environment with comprehensive domain randomization (physical properties, dynamics, noise injection) and train a neural network policy using Proximal Policy Optimization (PPO) with curriculum learning. Our experiments demonstrate that combining domain randomization with curriculum learning enables policies to generalize across varied environmental conditions, achieving 25\% success rate on reaching tasks at maximum difficulty. This work serves as a foundation for the FlexBot concept: using synthetic data generation to train embodied AI systems.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

\subsection{The Data Wall Problem}

Large Language Models (LLMs) have achieved remarkable success by training on vast amounts of internet text. However, \textbf{Embodied AI} (robots) faces a critical bottleneck: there is no equivalent "internet of physical interactions" from which to learn.

\begin{table}[H]
\centering
\caption{Data Availability Across AI Domains}
\begin{tabular}{lcc}
\toprule
\textbf{Domain} & \textbf{Training Data} & \textbf{Scale} \\
\midrule
Language Models & Internet text & Trillions of tokens \\
Vision Models & Web images (ImageNet) & Billions of images \\
\textbf{Embodied AI} & Robot experience & \textbf{Very limited} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Sim2Real Transfer}

\textbf{Sim2Real} transfer addresses this gap by training policies in simulation and deploying them on real robots. Key challenges include:

\begin{itemize}
    \item \textbf{Reality Gap}: Differences between simulated and real physics
    \item \textbf{Visual Discrepancy}: Simulated images differ from real camera feeds
    \item \textbf{Dynamics Mismatch}: Friction, inertia, and delays are hard to model perfectly
\end{itemize}

\subsection{Domain Randomization}

Domain Randomization (DR) bridges the reality gap by exposing the learning agent to a diverse range of randomized conditions during training, so that the real world appears as "just another variation" within the training distribution.

%==============================================================================
\section{Mathematical Framework}
%==============================================================================

\subsection{Robot Arm Kinematics}

For a planar robot arm with $n$ segments, the end-effector position is computed via \textbf{forward kinematics}:

\begin{equation}
\mathbf{p} = \sum_{i=1}^{n} l_i \begin{bmatrix} \cos\left(\sum_{j=1}^{i} \theta_j\right) \\ \sin\left(\sum_{j=1}^{i} \theta_j\right) \end{bmatrix}
\end{equation}

where:
\begin{itemize}
    \item $l_i$: Length of segment $i$
    \item $\theta_i$: Joint angle of segment $i$
    \item $\mathbf{p}$: End-effector position $(x, y)$
\end{itemize}

\subsection{Reinforcement Learning Formulation}

The reaching task is formulated as a Markov Decision Process (MDP):

\begin{itemize}
    \item \textbf{State}: $s = (\sin\theta, \cos\theta, \dot{\theta}, \mathbf{p}_{ee}, \mathbf{p}_{target}, d)$
    \item \textbf{Action}: $a = \Delta\theta$ (joint angle changes)
    \item \textbf{Reward}: $r = -d + r_{progress} + r_{success} + r_{energy}$
\end{itemize}

where $d = \|\mathbf{p}_{ee} - \mathbf{p}_{target}\|_2$ is the distance to target.

\subsection{Proximal Policy Optimization (PPO)}

PPO is a policy gradient method that constrains policy updates using a clipped objective:

\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]
\end{equation}

where:
\begin{itemize}
    \item $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$: Probability ratio
    \item $\hat{A}_t$: Advantage estimate (computed via GAE)
    \item $\epsilon$: Clipping parameter (typically 0.2)
\end{itemize}

\subsection{Generalized Advantage Estimation (GAE)}

GAE provides a balance between bias and variance in advantage estimation:

\begin{equation}
\hat{A}_t^{GAE(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
\end{equation}

where $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ is the TD residual.

%==============================================================================
\section{Domain Randomization Strategy}
%==============================================================================

Based on recent Sim2Real research, we implement comprehensive domain randomization across multiple categories:

\subsection{Physical Properties Randomization}

\begin{table}[H]
\centering
\caption{Domain Randomization Parameters}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Nominal} & \textbf{Range} \\
\midrule
Segment lengths & 0.25 m & $\pm$30\% \\
Segment masses & 1.0 kg & 0.5$\times$ -- 1.5$\times$ \\
Friction coefficient & 1.0 & $\pm$20\% \\
Torque limits & 1.0 & $\pm$20\% \\
\midrule
Action noise std. & 0 & 0.02 \\
Observation noise std. & 0 & 0.01 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Curriculum Learning}

We employ curriculum learning to gradually increase task difficulty:

\begin{equation}
\text{difficulty}(t) = \min\left(\frac{t}{0.7 \times T_{max}}, 1.0\right)
\end{equation}

where $t$ is the current episode and $T_{max}$ is the total number of training episodes. Randomization ranges scale with difficulty.

%==============================================================================
\section{Implementation}
%==============================================================================

\subsection{Environment}

\begin{itemize}
    \item \textbf{Robot}: 4-segment planar arm
    \item \textbf{State dimension}: 17 (angles, velocities, positions, distance)
    \item \textbf{Action dimension}: 4 (joint velocity commands)
    \item \textbf{Max episode length}: 150 steps
\end{itemize}

\subsection{Network Architecture}

\begin{algorithm}[H]
\caption{Actor-Critic Network}
\begin{algorithmic}[1]
\State \textbf{Shared Features:} $128 \rightarrow 128$ hidden units (ReLU)
\State \textbf{Actor Head:} $64 \rightarrow n_{actions}$ (Tanh)
\State \textbf{Learnable Log Std:} $\log\sigma$ per action dimension
\State \textbf{Critic Head:} $64 \rightarrow 1$ (Value function)
\end{algorithmic}
\end{algorithm}

\subsection{Training Hyperparameters}

\begin{table}[H]
\centering
\caption{PPO Training Hyperparameters}
\begin{tabular}{lc}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Learning rate & $3 \times 10^{-4}$ \\
Discount factor $\gamma$ & 0.99 \\
GAE $\lambda$ & 0.95 \\
Clip $\epsilon$ & 0.2 \\
Value coefficient & 0.5 \\
Entropy coefficient & 0.01 \\
PPO epochs per update & 4 \\
Batch size & 64 \\
Total episodes & 1000 \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Training Progression}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{FlexBot_V2_Results.png}
\caption{FlexBot V2 Training Results. Top row: Initial configuration, trained policy demonstration, and domain randomization scenarios. Middle row: Training reward curve, success rate over time, and evaluation performance. Bottom row: End-effector trajectories and conceptual diagrams for domain randomization and Sim2Real pipeline.}
\label{fig:results}
\end{figure}

\subsection{Performance Summary}

\begin{table}[H]
\centering
\caption{Training Results at Different Stages}
\begin{tabular}{lccc}
\toprule
\textbf{Episode} & \textbf{Avg Reward} & \textbf{Success Rate} & \textbf{Difficulty} \\
\midrule
100 & -85.94 & 12.0\% & 0.14 \\
300 & -72.58 & 13.0\% & 0.43 \\
500 & -72.26 & 10.0\% & 0.71 \\
700 & -65.26 & 20.0\% & 1.00 \\
1000 & -68.82 & 12.0\% & 1.00 \\
\midrule
\textbf{Eval (Det.)} & \textbf{-56.70} & \textbf{25.0\%} & 1.00 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Observations}

\begin{enumerate}
    \item \textbf{Curriculum Learning Works}: Success rate peaks at episode 700 when reaching full difficulty, showing the policy adapted to progressively harder tasks.
    
    \item \textbf{Deterministic Evaluation Outperforms}: The final evaluation (25\% success) exceeds training average (12\%), indicating learned policy is better than exploration-augmented training suggests.
    
    \item \textbf{Domain Randomization Enables Generalization}: The policy handles varied segment lengths, masses, and friction coefficients despite being trained only in simulation.
\end{enumerate}

%==============================================================================
\section{Comparison: V1 vs. V2}
%==============================================================================

\begin{table}[H]
\centering
\caption{Comparison of FlexBot Versions}
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{V1 (Basic)} & \textbf{V2 (Enhanced)} \\
\midrule
Algorithm & REINFORCE & PPO + GAE \\
Segments & 3 & 4 \\
Domain Randomization & Basic (position only) & Comprehensive \\
Curriculum Learning & No & Yes \\
Physics Model & Simple & Inertia + friction \\
Success Rate & 10\% & 25\% \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Sim2Real Transfer Considerations}
%==============================================================================

\subsection{Real Robot Deployment}

For actual Sim2Real transfer, additional steps would include:

\begin{enumerate}
    \item \textbf{System Identification}: Measure real robot dynamics
    \item \textbf{Fine-tuning}: Adapt policy on real robot with limited data
    \item \textbf{Safety Constraints}: Add collision avoidance, joint limits
    \item \textbf{Perception Integration}: Visual observation processing
\end{enumerate}

\subsection{Recommended Platforms}

\begin{itemize}
    \item \textbf{NVIDIA Isaac Sim}: High-fidelity physics, GPU-accelerated
    \item \textbf{MuJoCo}: Fast, accurate contact dynamics
    \item \textbf{PyBullet}: Open-source, Python-friendly
    \item \textbf{ROS2}: Real robot integration framework
\end{itemize}

%==============================================================================
\section{Future Directions}
%==============================================================================

\begin{itemize}
    \item Extend to 3D manipulation tasks (pick-and-place)
    \item Integrate visual observations (image-based control)
    \item Use adaptive domain randomization (AutoDR)
    \item Implement safe RL with constraint satisfaction
    \item Deploy on physical robot arm (UR5, Franka Panda)
\end{itemize}

%==============================================================================
\section{Conclusions}
%==============================================================================

This report demonstrated that:

\begin{enumerate}
    \item \textbf{Domain randomization} enables training robust robot control policies in simulation
    \item \textbf{PPO with GAE} provides stable reinforcement learning for continuous control
    \item \textbf{Curriculum learning} helps policies adapt to progressively harder tasks
    \item The combination achieves \textbf{2.5$\times$ improvement} over basic approaches
\end{enumerate}

This work establishes a foundation for the FlexBot vision: generating synthetic training data at scale to train embodied AI systems that can transfer to real-world robots.

%==============================================================================
\section{References}
%==============================================================================

\begin{enumerate}
    \item Schulman, J., et al. (2017). \textit{Proximal Policy Optimization Algorithms.} arXiv:1707.06347.
    
    \item Tobin, J., et al. (2017). \textit{Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.} IROS.
    
    \item OpenAI, et al. (2019). \textit{Learning Dexterous In-Hand Manipulation.} arXiv:1808.00177.
    
    \item Peng, X. B., et al. (2018). \textit{Sim-to-Real Robot Learning from Pixels with Progressive Nets.} CoRL.
    
    \item Muratore, F., et al. (2022). \textit{Robot Learning from Randomized Simulations: A Review.} Foundations and Trends in Robotics.
\end{enumerate}

%==============================================================================
\section{Appendix: Code Implementation}
%==============================================================================

The complete Python implementation is available in \texttt{flexbot\_v2\_demo.py}. Key components:

\begin{itemize}
    \item \texttt{RobotArmEnvV2}: Enhanced environment with domain randomization
    \item \texttt{ActorCritic}: PPO-style actor-critic network
    \item \texttt{compute\_gae}: Generalized Advantage Estimation
    \item \texttt{train\_ppo}: Main training loop with curriculum learning
\end{itemize}

\end{document}
