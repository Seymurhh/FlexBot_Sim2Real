\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

% Colors
\definecolor{codeblue}{RGB}{0,102,204}
\definecolor{codegray}{RGB}{128,128,128}

% Listings style
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{codeblue},
    commentstyle=\color{codegray},
    breaklines=true,
    frame=single,
    captionpos=b
}

% Title
\title{
    \textbf{FlexBot: Sim2Real Robot Control with Domain Randomization} \\
    \large A Technical Report on Synthetic Data Generation for Robot Learning
}
\author{
    Seymur Hasanov
}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
This report presents an exploration of \textbf{Sim2Real transfer} techniques for robot manipulation using domain randomization and reinforcement learning. We implement a simulated robot arm environment with comprehensive domain randomization (physical properties, dynamics, noise injection) and train a neural network policy using Proximal Policy Optimization (PPO) with curriculum learning. Our experiments demonstrate that combining domain randomization with curriculum learning enables policies to generalize across varied environmental conditions, achieving 25\% success rate on reaching tasks at maximum difficulty. We provide detailed analysis of training dynamics, identify challenges such as the difficulty-performance trade-off, and discuss strategies for improvement. This work serves as a foundation for the FlexBot concept: using synthetic data generation to train embodied AI systems.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

\subsection{The Data Wall Problem}

Large Language Models (LLMs) have achieved remarkable success by training on vast amounts of internet text---trillions of tokens scraped from the web. However, \textbf{Embodied AI} (robots that interact with the physical world) faces a critical bottleneck: there is no equivalent ``internet of physical interactions'' from which to learn.

\begin{table}[H]
\centering
\caption{Data Availability Across AI Domains}
\begin{tabular}{lcc}
\toprule
\textbf{Domain} & \textbf{Training Data} & \textbf{Scale} \\
\midrule
Language Models & Internet text & Trillions of tokens \\
Vision Models & Web images (ImageNet) & Billions of images \\
\textbf{Embodied AI} & Robot experience & \textbf{Very limited} \\
\bottomrule
\end{tabular}
\end{table}

Collecting real-world robot data is:
\begin{itemize}
    \item \textbf{Expensive}: Robots and infrastructure cost hundreds of thousands of dollars
    \item \textbf{Slow}: Real-time execution limits data collection speed
    \item \textbf{Dangerous}: Trial-and-error learning can damage equipment or injure humans
    \item \textbf{Limited in diversity}: Hard to create many scenarios in the real world
\end{itemize}

\subsection{Sim2Real Transfer}

\textbf{Sim2Real} transfer addresses this gap by training policies in simulation and deploying them on real robots. The key insight is that simulation provides:
\begin{itemize}
    \item \textbf{Infinite data}: Run millions of episodes in parallel
    \item \textbf{Safe exploration}: No risk of damage
    \item \textbf{Fast iteration}: 1000x faster than real-time
    \item \textbf{Scenario diversity}: Easily vary environments
\end{itemize}

However, Sim2Real faces the \textbf{reality gap}---differences between simulated and real physics:
\begin{itemize}
    \item \textbf{Dynamics mismatch}: Friction, inertia, and contact physics are hard to model
    \item \textbf{Visual discrepancy}: Rendered images differ from real camera feeds
    \item \textbf{Sensor noise}: Real sensors are noisier than simulated ones
\end{itemize}

\subsection{Domain Randomization}

Domain Randomization (DR) bridges the reality gap by training the agent on a \textbf{diverse distribution of environments}, so that the real world appears as ``just another variation'' within the training distribution.

%==============================================================================
\section{Mathematical Framework}
%==============================================================================

\subsection{Robot Arm Kinematics}

For a planar robot arm with $n$ segments, the end-effector position is computed via \textbf{forward kinematics}:

\begin{equation}
\mathbf{p} = \sum_{i=1}^{n} l_i \begin{bmatrix} \cos\left(\sum_{j=1}^{i} \theta_j\right) \\ \sin\left(\sum_{j=1}^{i} \theta_j\right) \end{bmatrix}
\end{equation}

where:
\begin{itemize}
    \item $l_i$: Length of segment $i$ (randomized during training)
    \item $\theta_i$: Joint angle of segment $i$ (controlled by the policy)
    \item $\mathbf{p}$: End-effector position $(x, y)$
\end{itemize}

\subsection{Reinforcement Learning Formulation}

The reaching task is formulated as a Markov Decision Process (MDP):

\begin{itemize}
    \item \textbf{State}: $s = (\sin\theta, \cos\theta, \dot{\theta}, \mathbf{p}_{ee}, \mathbf{p}_{target}, d)$
    \item \textbf{Action}: $a = \Delta\theta$ (joint velocity commands)
    \item \textbf{Reward}: $r = -d + r_{progress} + r_{success} + r_{energy}$
\end{itemize}

where $d = \|\mathbf{p}_{ee} - \mathbf{p}_{target}\|_2$.

\subsection{Proximal Policy Optimization (PPO)}

PPO constrains policy updates using a clipped objective:

\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]
\end{equation}

where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is the probability ratio.

\subsection{Generalized Advantage Estimation (GAE)}

GAE balances bias and variance in advantage estimation:

\begin{equation}
\hat{A}_t^{GAE(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
\end{equation}

where $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ is the TD residual.

%==============================================================================
\section{Domain Randomization Strategy}
%==============================================================================

\subsection{Randomization Parameters}

\begin{table}[H]
\centering
\caption{Domain Randomization Parameters}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Nominal} & \textbf{Range} \\
\midrule
Segment lengths & 0.25 m & $\pm$30\% \\
Segment masses & 1.0 kg & 0.5$\times$ -- 1.5$\times$ \\
Friction coefficient & 1.0 & $\pm$20\% \\
Torque limits & 1.0 & $\pm$20\% \\
\midrule
Action noise std. & 0 & 0.02 \\
Observation noise std. & 0 & 0.01 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Domain Randomization Visualization}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig_domain_randomization.png}
\caption{Domain randomization creates 25 different scenarios with varying arm lengths (workspace boundaries), target positions, and physical properties. The policy must learn to generalize across all these variations.}
\label{fig:domain_rand}
\end{figure}

\textbf{Analysis}: Figure \ref{fig:domain_rand} shows the diversity of training scenarios. Each blue arc represents a different workspace boundary (due to varying segment lengths), and each red star is a different target position. This diversity forces the policy to learn invariant features rather than memorizing a single solution.

\subsection{Curriculum Learning}

Difficulty increases linearly during training:

\begin{equation}
\text{difficulty}(t) = \min\left(\frac{t}{0.7 \times T_{max}}, 1.0\right)
\end{equation}

This allows the agent to first master easy scenarios before facing the full randomization challenge.

%==============================================================================
\section{Results and Analysis}
%==============================================================================

\subsection{Training Reward Progression}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_reward_curve.png}
\caption{Training reward over episodes (50-episode moving average). Orange vertical line marks early exploration phase; green line marks when curriculum reaches maximum difficulty.}
\label{fig:reward}
\end{figure}

\textbf{Analysis}: Figure \ref{fig:reward} shows the reward monotonically improving from -90 to approximately -60. Key observations:

\begin{itemize}
    \item \textbf{Early exploration (episodes 0-200)}: Reward improves rapidly as the policy learns basic reaching behavior in easy environments.
    \item \textbf{Curriculum transition (episodes 200-700)}: Improvement slows as difficulty increases---the policy must continuously adapt to harder scenarios.
    \item \textbf{Maximum difficulty (episodes 700+)}: Reward stabilizes. The policy has found a local optimum given the training dynamics.
\end{itemize}

The plateau at -60 reward suggests the policy achieves moderate proximity to targets (average distance $\sim$0.4) but doesn't consistently reach them (threshold is 0.05).

\subsection{Success Rate Analysis}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_success_rate.png}
\caption{Success rate over training. Green line: training success rate (100-episode average). Red markers: evaluation success rate (deterministic policy). Dashed line: curriculum difficulty.}
\label{fig:success}
\end{figure}

\textbf{Analysis}: Figure \ref{fig:success} reveals the \textbf{difficulty-performance trade-off}:

\begin{itemize}
    \item Success rate peaks around 20-25\% at episode 700-800
    \item Evaluation success (deterministic policy) consistently exceeds training success, indicating exploration noise hurts accuracy
    \item As difficulty reaches 100\% (dashed line), success rate plateaus
\end{itemize}

\textbf{Why 25\% is actually reasonable}:
\begin{enumerate}
    \item The task becomes significantly harder at high difficulty (targets farther, more randomization)
    \item Real-world Sim2Real papers often report similar ranges before fine-tuning
    \item The policy generalizes across 1000s of different scenarios, not just one
\end{enumerate}

\subsection{Distance to Target}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig_distance.png}
\caption{Average final distance to target over training (100-episode moving average). Red dashed line indicates success threshold (0.05).}
\label{fig:distance}
\end{figure}

\textbf{Analysis}: Figure \ref{fig:distance} shows distance decreasing from 0.8 to approximately 0.4. The success threshold is 0.05, which explains the 25\% success rate---the policy often gets \textit{close} but not \textit{precise}.

\textbf{Improvement strategies}:
\begin{itemize}
    \item \textbf{Finer control}: Increase action resolution near targets
    \item \textbf{Two-phase policy}: Coarse reaching + fine adjustment
    \item \textbf{Hindsight Experience Replay}: Learn from near-misses
\end{itemize}

\subsection{Policy Demonstrations}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig_demonstrations.png}
\caption{Six policy demonstrations at maximum difficulty. Green dashed lines show end-effector trajectories. Checkmarks indicate successful reaches (distance $<$ 0.05).}
\label{fig:demo}
\end{figure}

\textbf{Analysis}: Figure \ref{fig:demo} shows typical policy behavior:
\begin{itemize}
    \item The arm moves \textit{toward} targets (trajectories converge)
    \item Success depends on target location and arm configuration
    \item Failures often result from getting stuck in local minima
\end{itemize}

\subsection{End-Effector Trajectories}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig_trajectories.png}
\caption{End-effector trajectories across 10 different randomized scenarios. Stars indicate targets; circles indicate final positions.}
\label{fig:traj}
\end{figure}

\textbf{Analysis}: Figure \ref{fig:traj} demonstrates that the policy generalizes across workspace:
\begin{itemize}
    \item Trajectories show directed motion toward targets (not random)
    \item Both near and far targets are attempted
    \item The policy adapts to different arm configurations
\end{itemize}

%==============================================================================
\section{Why Training Performance Plateaus}
%==============================================================================

The 25\% success rate reflects several interacting challenges:

\subsection{Challenge 1: Exploration-Exploitation Trade-off}

PPO uses stochastic policies for exploration, but this adds noise that hurts precision near targets. The evaluation policy (deterministic) achieves higher success because it eliminates this noise.

\textbf{Solution}: Use entropy annealing---high entropy early, low entropy late.

\subsection{Challenge 2: Curriculum Learning Tension}

As difficulty increases, previously-learned behaviors may become suboptimal for new scenarios. The policy must unlearn and relearn, which slows progress.

\textbf{Solution}: Continual Domain Randomization (CDR)---introduce parameters gradually rather than all at once.

\subsection{Challenge 3: Sparse Rewards}

Success only occurs when distance $<$ 0.05. Most episodes end without success signal, making credit assignment difficult.

\textbf{Solution}: 
\begin{itemize}
    \item Hindsight Experience Replay (HER)
    \item Denser reward shaping near targets
    \item Goal-conditioned policies
\end{itemize}

\subsection{Challenge 4: High-Dimensional Randomization}

Randomizing 6+ parameters simultaneously creates a vast space. The policy may not encounter enough samples from each ``corner'' of this space.

\textbf{Solution}: Automatic Domain Randomization (ADR)---let the agent control difficulty.

%==============================================================================
\section{Comparison: Baseline vs. PPO}
%==============================================================================

\begin{table}[H]
\centering
\caption{Comparison of Training Approaches}
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{V1 (REINFORCE)} & \textbf{V2 (PPO + DR)} \\
\midrule
Algorithm & REINFORCE & PPO + GAE \\
Segments & 3 & 4 \\
Domain Randomization & Basic & Comprehensive \\
Curriculum Learning & No & Yes \\
Max Success Rate & 10\% & 25\% \\
Training Stability & Low & High \\
\bottomrule
\end{tabular}
\end{table}

The 2.5$\times$ improvement demonstrates that PPO, domain randomization, and curriculum learning work synergistically.

%==============================================================================
\section{Sim2Real Transfer Considerations}
%==============================================================================

For actual deployment on a real robot:

\begin{enumerate}
    \item \textbf{System Identification}: Measure real robot dynamics and tune simulation
    \item \textbf{Fine-tuning}: Adapt policy on real robot with limited data (10-100 episodes)
    \item \textbf{Safety Constraints}: Add collision avoidance, joint limits, torque limits
    \item \textbf{Visual Domain Randomization}: For camera-based control
\end{enumerate}

\subsection{Recommended Platforms}
\begin{itemize}
    \item \textbf{NVIDIA Isaac Sim}: High-fidelity physics, GPU-accelerated, supports domain randomization
    \item \textbf{MuJoCo}: Fast, accurate contact dynamics
    \item \textbf{ROS2}: Real robot integration
\end{itemize}

%==============================================================================
\section{Future Work}
%==============================================================================

\begin{enumerate}
    \item Implement Hindsight Experience Replay (HER) for better sample efficiency
    \item Add visual observations (image-based control)
    \item Use adaptive domain randomization (AutoDR)
    \item Deploy on physical robot (UR5, Franka Panda)
    \item Extend to 3D manipulation (6-DOF)
\end{enumerate}

%==============================================================================
\section{Conclusions}
%==============================================================================

This exploration demonstrated:

\begin{enumerate}
    \item \textbf{Domain randomization} enables training robust policies that generalize across varied physical parameters
    \item \textbf{PPO with GAE} provides stable reinforcement learning for continuous robot control
    \item \textbf{Curriculum learning} helps policies adapt to progressively harder tasks
    \item The combination achieves \textbf{2.5$\times$ improvement} over baseline REINFORCE
    \item A \textbf{25\% success rate} at maximum difficulty is reasonable given the diversity of scenarios
\end{enumerate}

The training dynamics reveal important insights: success rate plateaus due to exploration-exploitation trade-offs, curriculum tension, and sparse rewards. Future work addressing these challenges through HER, entropy annealing, and adaptive randomization could further improve performance.

This work establishes a foundation for the FlexBot vision: generating synthetic training data at scale to train embodied AI systems that can transfer to real-world robots.

%==============================================================================
\section{References}
%==============================================================================

\begin{enumerate}
    \item Schulman, J., et al. (2017). \textit{Proximal Policy Optimization Algorithms.} arXiv:1707.06347.
    
    \item Tobin, J., et al. (2017). \textit{Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.} IROS.
    
    \item OpenAI, et al. (2019). \textit{Learning Dexterous In-Hand Manipulation.} arXiv:1808.00177.
    
    \item Andrychowicz, M., et al. (2017). \textit{Hindsight Experience Replay.} NeurIPS.
    
    \item Akkaya, I., et al. (2019). \textit{Solving Rubik's Cube with a Robot Hand.} arXiv:1910.07113.
\end{enumerate}

%==============================================================================
\section{Appendix: Code Implementation}
%==============================================================================

The complete implementation is available in:
\begin{itemize}
    \item \texttt{flexbot\_v2\_demo.py}: PPO training with domain randomization
    \item \texttt{generate\_figures.py}: Report figure generation
\end{itemize}

Key hyperparameters:
\begin{lstlisting}
LEARNING_RATE = 3e-4
GAMMA = 0.99
GAE_LAMBDA = 0.95
CLIP_EPSILON = 0.2
N_EPISODES = 1000
\end{lstlisting}

\end{document}
